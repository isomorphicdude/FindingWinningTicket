{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "A notebook to test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, we use the `MNIST` data. We first extract the data from `gzip` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return (tf.cast(image, tf.float32)/255.0 , label)  \n",
    "\n",
    "def preProcess(dset, batch_size):\n",
    "  '''Pre-processes the dataset.'''\n",
    "  dset = dset.map(normalize_img)\n",
    "  dset = dset.shuffle(len(dset))\n",
    "  dset = dset.batch(batch_size)\n",
    "  return dset\n",
    "  \n",
    "# prepare data\n",
    "def get_imgs(dir):\n",
    "    with gzip.open(dir, 'r') as f:\n",
    "        # first 4 bytes (some number)\n",
    "        _ = int.from_bytes(f.read(4), 'big')\n",
    "        # no. of images\n",
    "        num_imgs = int.from_bytes(f.read(4), 'big')\n",
    "        # row count\n",
    "        row_cnt = int.from_bytes(f.read(4), 'big')\n",
    "        # column count\n",
    "        col_cnt = int.from_bytes(f.read(4), 'big')\n",
    "\n",
    "        img_data = f.read()\n",
    "        images = np.frombuffer(img_data, dtype=np.uint8).\\\n",
    "            reshape((num_imgs, row_cnt, col_cnt))\n",
    "        return images\n",
    "\n",
    "def get_labels(dir):\n",
    "    with gzip.open(dir, 'r') as f:\n",
    "        _ = int.from_bytes(f.read(4), 'big')\n",
    "        label_cnt = int.from_bytes(f.read(4), 'big')\n",
    "        print(label_cnt)\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make `dataset`. Note that we used the `tf.data.Dataset.from_tensor_slices` to create dataset from a tuple of numpy arrays; if they were instead lists, then there will be an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocalDset(batch_size = 128):\n",
    "    imgs     = get_imgs('data/train-images-idx3-ubyte.gz')\n",
    "    labels   = get_labels('data/train-labels-idx1-ubyte.gz')\n",
    "    split    = int(0.75*len(imgs))\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((imgs[:split], labels[:split]))\n",
    "    ds_test  = tf.data.Dataset.from_tensor_slices((imgs[split:], labels[split:]))\n",
    "    print(len(ds_train))\n",
    "    print(len(ds_test))\n",
    "    return preProcess(ds_train, batch_size), preProcess(ds_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the data by downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_img(ds, batch_size = 128):\n",
    "  ds = ds.cache()\n",
    "  # ds = ds.shuffle(ds_info.splits['train'].num_examples)\n",
    "  ds = ds.batch(batch_size)\n",
    "  # ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def getRemoteDset(batch_size = 128, name = 'MNIST'):\n",
    "\n",
    "  '''Prepares the MNIST dataset for training.'''  \n",
    "\n",
    "  if name == 'MNIST':\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    # split=['test', 'train[0%:10%]','train[10%:]'],\n",
    "    split = ['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info = True\n",
    "    )\n",
    "\n",
    "  ds_train = ds_train.map(normalize_img, \n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  ds_test = ds_test.map(normalize_img, \n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  ds_train = fetch_img(ds_train, batch_size)\n",
    "  ds_test = fetch_img(ds_test, batch_size)\n",
    "\n",
    "  return (ds_train, ds_test), ds_info\n",
    "\n",
    "(ds_train, ds_test), ds_info = getRemoteDset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Running experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [300, 100, 10] # use [784, 100, 10]\n",
    "\n",
    "activation = 'relu'\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metrics = tf.keras.metrics.SparseCategoricalAccuracy()    \n",
    "\n",
    "train_loss = tf.keras.metrics.Mean\n",
    "\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy  \n",
    "\n",
    "epochs = 5\n",
    "\n",
    "num_pruning = 5  \n",
    "\n",
    "step_perc = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ModeltoPrune\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 0 for original\n",
      "OG train acc 0.9221500158309937\n",
      "Original model accuracy 0.9592000246047974 \n",
      "\n",
      "Epoch 1 for original\n",
      "OG train acc 0.9704166650772095\n",
      "Original model accuracy 0.9711999893188477 \n",
      "\n",
      "Epoch 2 for original\n",
      "OG train acc 0.9814333319664001\n",
      "Original model accuracy 0.9725000262260437 \n",
      "\n",
      "Epoch 3 for original\n",
      "OG train acc 0.9870666861534119\n",
      "Original model accuracy 0.9735999703407288 \n",
      "\n",
      "Epoch 4 for original\n",
      "OG train acc 0.991599977016449\n",
      "Original model accuracy 0.9735999703407288 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9735999703407288 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pruning import *\n",
    "from models import *\n",
    "\n",
    "model_params = {'layers': layers,\n",
    "                'activation': 'relu',\n",
    "                'BatchNorm':True,\n",
    "                'Dropout':[0.5, 0.5, 0.5],\n",
    "                'optimizer': tf.keras.optimizers.Adam(lr),\n",
    "                'loss_fn': loss_fn,\n",
    "                'metrics': metrics\n",
    "                }\n",
    "\n",
    "train_params = {'train_loss': train_loss,\n",
    "                'train_acc': train_acc\n",
    "                }\n",
    "\n",
    "prune_exp = pruning(ds_train, ds_test, model_params,\n",
    "                    train_params,\n",
    "                    epochs, num_pruning, step_perc)\n",
    "                    \n",
    "prune_exp.test_training()\n",
    "# prune_exp.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Iterative pruning round: 0 \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 266200 \n",
      " \n",
      "\n",
      "\n",
      " Start original model training. \n",
      "\n",
      "Epoch 0 for original\n",
      "OG train acc 0.9224333167076111\n",
      "Original model accuracy 0.9603000283241272 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9603000283241272 \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 133100 \n",
      " \n",
      "\n",
      "\n",
      " Start Lottery ticket training \n",
      "\n",
      "Epoch 0 for lottery ticket\n",
      "Ticket train acc 0.8335000276565552 \n",
      "\n",
      "Lottery ticket accuracy 0.9172000288963318 \n",
      "\n",
      "\n",
      " Lottery ticket training finished. Highest Accuracy 0.9172000288963318 \n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9172000288963318 pruning round_0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9172000288963318 pruning round_0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Iterative pruning round: 1 \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 133100 \n",
      " \n",
      "\n",
      "\n",
      " Start original model training. \n",
      "\n",
      "Epoch 0 for original\n",
      "OG train acc 0.9265333414077759\n",
      "Original model accuracy 0.9628000259399414 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9628000259399414 \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 67096 \n",
      " \n",
      "\n",
      "\n",
      " Start Lottery ticket training \n",
      "\n",
      "Epoch 0 for lottery ticket\n",
      "Ticket train acc 0.881850004196167 \n",
      "\n",
      "Lottery ticket accuracy 0.9323999881744385 \n",
      "\n",
      "\n",
      " Lottery ticket training finished. Highest Accuracy 0.9323999881744385 \n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9323999881744385 pruning round_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9323999881744385 pruning round_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Iterative pruning round: 2 \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 67096 \n",
      " \n",
      "\n",
      "\n",
      " Start original model training. \n",
      "\n",
      "Epoch 0 for original\n",
      "OG train acc 0.910183310508728\n",
      "Original model accuracy 0.9506999850273132 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9506999850273132 \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 35354 \n",
      " \n",
      "\n",
      "\n",
      " Start Lottery ticket training \n",
      "\n",
      "Epoch 0 for lottery ticket\n",
      "Ticket train acc 0.8773666620254517 \n",
      "\n",
      "Lottery ticket accuracy 0.9261000156402588 \n",
      "\n",
      "\n",
      " Lottery ticket training finished. Highest Accuracy 0.9261000156402588 \n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9261000156402588 pruning round_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9261000156402588 pruning round_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Iterative pruning round: 3 \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 35354 \n",
      " \n",
      "\n",
      "\n",
      " Start original model training. \n",
      "\n",
      "Epoch 0 for original\n",
      "OG train acc 0.8942000269889832\n",
      "Original model accuracy 0.9391000270843506 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9391000270843506 \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 19708 \n",
      " \n",
      "\n",
      "\n",
      " Start Lottery ticket training \n",
      "\n",
      "Epoch 0 for lottery ticket\n",
      "Ticket train acc 0.8580499887466431 \n",
      "\n",
      "Lottery ticket accuracy 0.9132999777793884 \n",
      "\n",
      "\n",
      " Lottery ticket training finished. Highest Accuracy 0.9132999777793884 \n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9132999777793884 pruning round_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.9132999777793884 pruning round_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Iterative pruning round: 4 \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 19708 \n",
      " \n",
      "\n",
      "\n",
      " Start original model training. \n",
      "\n",
      "Epoch 0 for original\n",
      "OG train acc 0.8436166644096375\n",
      "Original model accuracy 0.9175999760627747 \n",
      "\n",
      "\n",
      " Original model training finished. Highest Accuracy 0.9175999760627747 \n",
      "\n",
      "\n",
      " \n",
      " After pruning, the total no. of nonzero weights is: 11254 \n",
      " \n",
      "\n",
      "\n",
      " Start Lottery ticket training \n",
      "\n",
      "Epoch 0 for lottery ticket\n",
      "Ticket train acc 0.7550333142280579 \n",
      "\n",
      "Lottery ticket accuracy 0.8676999807357788 \n",
      "\n",
      "\n",
      " Lottery ticket training finished. Highest Accuracy 0.8676999807357788 \n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.8676999807357788 pruning round_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/ticket_acc_0.8676999807357788 pruning round_4\\assets\n"
     ]
    }
   ],
   "source": [
    "prune_exp.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 20220806-130249\n",
      "Removing 20220806-130748\n",
      "Removing 20220806-131740\n"
     ]
    }
   ],
   "source": [
    "prune_exp.removeLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ticket_acc_0.8676999807357788 pruning round_4\n",
      "Removing ticket_acc_0.9132999777793884 pruning round_3\n",
      "Removing ticket_acc_0.9172000288963318 pruning round_0\n",
      "Removing ticket_acc_0.9261000156402588 pruning round_2\n",
      "Removing ticket_acc_0.9323999881744385 pruning round_1\n"
     ]
    }
   ],
   "source": [
    "prune_exp.removeModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "214ae085943121352a1c46be52870768915bb1800a79b1ca0ca08a53dfbff110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
